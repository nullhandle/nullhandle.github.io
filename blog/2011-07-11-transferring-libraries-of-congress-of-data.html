<!DOCTYPE html>
<html lang="en">
  <head prefix="dc: http://purl.org/dc/elements/1.1 dcterms: http://purl.org/dc/terms/">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <link href="http://purl.org/dc/elements/1.1/" rel="schema.dc">
    <link href="http://purl.org/dc/terms/" rel="schema.dcterms">
    <link href="../css/responsive-nav.css" rel="stylesheet" type="text/css">
    <link href="../css/stylesheet.css" rel="stylesheet" type="text/css">
    <link href="../img/favicon.ico" rel="icon" type="image/x-icon">
    <link href="feed.xml" rel="alternate" title="nullhandle.org" type="application/rss+xml">
    <meta charset="UTF-8">
    <meta name="DC.Contributor.PersonalName" content="Mike Ashenfelder">
    <meta name="DC.Creator.PersonalName" content="Nicholas Taylor">
    <meta name="DC.Date" content="2011-07-11" scheme="DCTERMS.W3CDTF">
    <meta name="DC.Format" content="text/html" scheme="DCTERMS.IMT">
    <meta name="DC.Language" content="en-US" scheme="DCTERMS.RFC4646">
    <meta name="DC.Rights" content="https://creativecommons.org/licenses/by-sa/4.0/" scheme="DCTERMS.URI">
    <meta name="DC.Source" content="https://nullhandle.org/" scheme="DCTERMS.URI">
    <meta name="DC.Title" content='Transferring "Libraries of Congress" of Data'>
    <meta name="DC.Type" content="blogPost">
    <!-- Cloudflare Web Analytics --><script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"token": "514c441e9d464f3fb864fa3a50ae9d62"}'></script><!-- End Cloudflare Web Analytics -->
    <script src="../js/responsive-nav.js"></script>
    <title>Transferring "Libraries of Congress" of Data</title>
  </head>
  <body itemscope itemtype="http://schema.org/WebPage">
    <div class="border"></div>
    <nav class="nav-collapse" id="nav">
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="../resume.html">CV</a></li>
        <li><a href="../consulting.html">Consulting</a></li>
        <li class="active"><a href="index.html">Blog</a></li>
        <li><a href="../web-archivability/index.html">Archivability</a></li>
      </ul>
    </nav>
    <div class="content" itemscope itemtype="http://schema.org/BlogPosting">
      <h2 itemprop="headline">Transferring "Libraries of Congress" of Data</h2>
      <div class="byline">Published by <span itemprop="author publisher" itemscope itemtype="http://schema.org/Person"><span itemprop="name" rel="author"><a href="../index.html" itemprop="url"><span itemprop="givenName">Nicholas</span> <span itemprop="familyName">Taylor</span></a></span></span> on <time datetime="2017-09-29T19:18:00-07:00" itemprop="dateModified"></time><time datetime="2011-07-11T11:51:56-04:00" itemprop="datePublished">11 July 2011</time></div>
      <br>
      <div itemprop="articleBody">
        <div>If science reporters, IT industry pundits and digital storage and network infrastructure purveyors <a href="https://newsroom.cisco.com/press-release-content?type=webcontent&amp;articleId=5386763">are</a> <a data-originalurl="http://enterprise.media.seagate.com/2010/12/inside-it-storage/future-storage-technology-how-to-store-the-entire-u-s-library-of-congress-on-a-coin/" data-versiondate="2011-07-17" href="https://web.archive.org/web/20110717080100/http://enterprise.media.seagate.com/2010/12/inside-it-storage/future-storage-technology-how-to-store-the-entire-u-s-library-of-congress-on-a-coin/">to</a> <a href="http://www.opternity.com/">be</a> <a href="https://www.baltimoresun.com/2011/05/07/md-based-intelligence-agencies-helped-track-bin-laden-2/">believed</a>, devices are being lab-tested even now that can store all of the data in the Library of Congress or transmit it over a network in mere moments. To this list of improbable claims, I'd like to add another: by the most conservative estimates, I transfer more than a Library of Congress' worth of data to the Library of Congress every month.</div>
        <br>
        <figure class="figure-left" itemprop="image" itemscope itemtype="https://schema.org/ImageObject" xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
          <img alt="Library of Congress" height="320" itemprop="url" src="https://c5.staticflickr.com/3/2389/1659354500_76a665af1c_n.jpg" width="251">
          <meta itemprop="height" content="320">
          <meta itemprop="width" content="251">
          <figcaption>"<a href="https://www.flickr.com/photos/mysterybee/1659354500/" property="dct:title">Library of Congress</a>" by <a href="https://www.flickr.com/photos/mysterybee/" property="cc:attributionName" rel="cc:attributionURL dct:creator">Henrik Bennetsen</a> under <a href="https://creativecommons.org/licenses/by-sa/2.0/" rel="license">CC BY-SA 2.0</a></figcaption>
        </figure>
        <div>Clearly, that doesn't make any sense, but allow me to explain. You may have noticed that the "data stored by the Library of Congress" has become a popular, if unusual, unit of measurement for capacity (and the subject of a previous Library of Congress blog <a href="https://blogs.loc.gov/loc/2009/02/how-big-is-the-library-of-congress/">post</a>, to boot). More cautious commentators instead employ the "data represented by the digitized print collections of the Library of Congress." My non-exhaustive research (nonetheless, <a href="https://en.wikipedia.org/w/index.php?title=List_of_unusual_units_of_measurement&amp;oldid=437780824#Encyclopaediae.2C_Bibles.2C_and_the_Library_of_Congress:_data_storage_capacities">corroborated</a> by Wikipedia) suggests that in instances where a specific number is quoted, that number is most frequently 10 terabytes (and, in a curious bit of self-referentiality, the Library of Congress Web Archiving program is <a href="https://en.wikipedia.org/w/index.php?title=Terabyte&amp;oldid=438715234#Illustrative_usage_examples">referenced</a> in Wikipedia to help illustrate what a "Terabyte"" is). From whither, 10 terabytes?</div>
        <br>
        <div>The earliest authoritative reference to the 10 terabytes number comes from an ambitious 2000 <a data-originalurl="http://www2.sims.berkeley.edu/research/projects/how-much-info/" data-versiondate="2011-07-18" href="https://web.archive.org/web/20110718205552/http://www2.sims.berkeley.edu/research/projects/how-much-info/">study</a> by <a href="https://www.ischool.berkeley.edu/">UC Berkeley iSchool</a> professors <a data-originalurl="https://www.ischool.berkeley.edu/people/faculty/peterlyman" data-versiondate="2011-07-17" href="https://web.archive.org/web/20110717063927/https://www.ischool.berkeley.edu/people/faculty/peterlyman">Peter Lyman</a> and <a data-originalurl="https://people.ischool.berkeley.edu/~hal/people/hal/biography.html" data-versiondate="2011-07-31" href="https://web.archive.org/web/20110731010520/https://people.ischool.berkeley.edu/~hal/people/hal/biography.html">Hal Varian</a> which attempted to measure how much information was produced in the world that year. In it, <a data-originalurl="http://www2.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=110" data-versiondate="2010-06-26" href="https://web.archive.org/web/20100626020558/http://www.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=110">they note</a> with little fanfare that 10 terabytes is the size of the Library of Congress print collections. They subsequently elaborate their assumptions in an <a data-originalurl="http://www2.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=112" data-versiondate="2010-06-26" href="https://web.archive.org/web/20100626020558/http://www.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=112">appendix</a>: the average book has 300 pages, is scanned as a 600 <a href="https://en.wikipedia.org/w/index.php?title=Dots_per_inch&amp;oldid=437743510#DPI_or_PPI_in_digital_image_files">DPI</a> <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000022.shtml">TIFF</a>, and, finally, compressed, resulting in an estimated size of 8 megabytes per book. At the time of the study's publication, they <a data-originalurl="http://www2.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=14" data-versiondate="2010-06-26" href="https://web.archive.org/web/20100626020558/http://www.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=14">supposed</a> that the Library of Congress print collections consisted of 26 million books. Even taking these assumptions for granted, the <a href="https://www.google.com/search?hl=&amp;q=8+megabytes+x+26+million">math</a> yields a number much closer to 200 terabytes. Sure enough, the authors <a data-originalurl="http://www2.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=14" data-versiondate="2010-06-26" href="https://web.archive.org/web/20100626020558/http://www.sims.berkeley.edu/research/projects/how-much-info/how-much-info.pdf#page=14">note</a> parenthetically elsewhere in the study that the size of the Library of Congress print collections is 208 terabytes. No explanation is offered for the discrepancy with the other quoted number.</div>
        <br>
        <figure class="figure-right" itemprop="image" itemscope itemtype="https://schema.org/ImageObject" xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
          <img alt="DATABASE at Postmasters, March 2009" height="213" src="https://c2.staticflickr.com/4/3560/3332644561_c9d5041d02_n.jpg" width="320">
          <meta itemprop="height" content="213">
          <meta itemprop="width" content="320">
          <figcaption>"<a href="https://www.flickr.com/photos/theredproject/3332644561" property="dct:title">DATABASE at Postmasters, March 2009</a>" by <a href="https://www.flickr.com/photos/theredproject/" property="cc:attributionName" rel="cc:attributionURL dct:creator">Michael Mandiberg</a> under <a href="https://creativecommons.org/licenses/by-sa/2.0/" rel="license">CC BY-SA 2.0</a></figcaption>
        </figure>
        <div>For whatever reason, though, it's the 10 terabyte figure that took hold in the public's imagination. To be sure, 10 terabytes is an impressive amount of data, but it's far less impressive than the amount of data that the Library of Congress actually contains (and, I suspect, even just counting the print collections). While I'm neither clever nor na&#xEF;ve enough to propose what a more realistic number might be, returning to my original provocation, I did wish to further discuss a digital collection I know quite well: <a href="https://www.loc.gov/web-archives/collections/">the Library of Congress Web Archives</a>.</div>
        <br>
        <div>As explained <a href="https://blogs.loc.gov/thesignal/2011/06/the-first-decade-of-web-archiving-at-the-library-of-congress/">previously</a> in The Signal, we currently contract with the <a href="https://www.archive.org/">Internet Archive</a> to perform our large-scale web crawling. One ancillary task that arises from this arrangement is that the generated web archive data (roughly 5 terabytes per month) must be transferred from the West Coast to the Library of Congress. This turns out to be non-trivial; it may take the better part of a month with near-constant transfers over an <a href="https://internet2.edu/">Internet2</a> connection to move 10 terabytes of data. For all the optimism about transmitting "Libraries of Congress" of data over networks, putting data on physical storage media and then shipping that media around remains a surprisingly <a href="https://en.wikipedia.org/w/index.php?title=Sneakernet&amp;oldid=437421029#Theory">competitive alternative</a>. Case in point: for all of the ethereality and technological sophistication implied by so-called <a href="https://en.wikipedia.org/w/index.php?title=Cloud_computing&amp;oldid=438738986">cloud</a> services, at least one of the major providers lets users upload their data in the comparatively mundane manner of <a data-originalurl="https://aws.amazon.com/importexport/?ref_=pe_12300_20444540" data-versiondate="2011-07-11" href="https://web.archive.org/web/20110711110436/https://aws.amazon.com/importexport/?ref_=pe_12300_20444540">mailing a hard drive</a>.</div>
        <br>
        <div>Of course, transfer is just the initial stage in our management of the web archive data; the infrastructure demands compound when you consider the requirements for redundant storage on tape and/or spinning disk, internal network bandwidth, and processor cycles for copying, indexing, validation, and so forth. In summary, I doubt that we have spare capacity to store and process many more "Libraries of Congress" of data than we are currently (though perhaps that's <a href="https://en.wikipedia.org/w/index.php?title=Tautology_%28logic%29&amp;oldid=435554692">self-evident</a>).</div>
        <br>
        <div>Suffice it to say, I look forward to a day when IT hardware manufacturers can legitimately claim to handle magnitudes of data commensurate with what is actually stored within the Library of Congress (whatever that amount may be). In the meantime, however, I suppose I'd settle for the popular adoption of fractional &#8220;Library of Congress&#8221; units of capacity (e.g., &#8220;.000001% of the data stored at the Library of Congress&#8221;) &#8211; likely no more or less realistic than what the actual number might be, but at least it'd more appropriately aggrandize just how much data the Library of Congress has.</div>
      </div>
      <br>
      <div class="byline"><a href="https://nullhandle.org/blog/2011-07-11-transferring-libraries-of-congress-of-data.html" rel="canonical">Permalink</a> | <a href="https://blogs.loc.gov/thesignal/2011/07/transferring-libraries-of-congress-of-data/">Crossposted</a> to <a href="https://blogs.loc.gov/thesignal/">The Signal</a><a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license"><img alt="Creative Commons Attribution-ShareAlike 4.0 International License" class="copyright" height="15" src="https://licensebuttons.net/l/by-sa/4.0/80x15.png" width="80"></a></div>
      <br>
      <script>
        var nav = responsiveNav(".nav-collapse");
      </script>
    </div>
  </body>
</html>