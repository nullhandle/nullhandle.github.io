<!DOCTYPE html>
<html lang="en">
  <head>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="../css/responsive-nav.css" rel="stylesheet" type="text/css">
    <link href="../css/stylesheet.css" rel="stylesheet" type="text/css">
    <link href="../img/favicon.ico" rel="icon" type="image/x-icon">
    <meta charset="UTF-8">
    <meta name="description" content="">
    <!-- Cloudflare Web Analytics --><script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"token": "514c441e9d464f3fb864fa3a50ae9d62"}'></script><!-- End Cloudflare Web Analytics -->
    <script src="js/responsive-nav.js"></script>
    <title>web archivability > capture - nullhandle.org</title>
</head>
  <body itemscope itemtype="https://schema.org/Person">
    <div class="border"></div>
    <nav class="nav-collapse" id="nav">
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="../resume.html">CV</a></li>
        <li><a href="../consulting.html">Consulting</a></li>
        <li><a href="../blog/index.html">Blog</a></li>
        <li class="active"><a href="index.html">Archivability</a></li>
      </ul>
    </nav>
    <div class="content">
      <h1><a href="index.html">Web Archivability</a></h1>
      <h2>Capture</h2>
      <div>Improving the ability of archival crawlers to capture your website will also tend to improve discovery of your content by search engine crawlers, enhance website performance by decreasing the load caused by robots and other clients, and save storage space.</div>
      <h3>Make links transparent</h3>
      <div>An archival crawler <a href="https://en.wikipedia.org/wiki/Web_crawler#Overview">finds your content by following links</a>, typically starting from the home page. However, it can't archive what it hasn't discovered. Links that are dependent on <a href="https://sighjavascript.tumblr.com/">JavaScript execution</a> or embedded in binary files (e.g., <a href="https://en.wikipedia.org/wiki/Adobe_Flash">Flash</a>, <a href="https://en.wikipedia.org/wiki/PDF">PDF</a>, <a href="https://en.wikipedia.org/wiki/Microsoft_Word#File_formats">Word documents</a>, <a href="https://en.wikipedia.org/wiki/Microsoft_Excel#File_formats">Excel spreadsheets</a>, etc.) tend to be opaque to an archival crawler, so ensure that they are additionally discoverable in a way that doesn't depend on those technologies. You can affirmatively improve the discoverability of your content by enumerating the resources you'd like crawlers to know about in an <a href="https://www.sitemaps.org/">XML sitemap</a>, a <a href="https://www.nngroup.com/articles/site-map-usability/">user sitemap</a>, and/or <a href="https://en.wikipedia.org/wiki/RSS">RSS feeds</a>.</div>
      <h3>Represent web app states with links</h3>
      <div>The breadth and complexity of a modern <a href="https://en.wikipedia.org/wiki/Web_application">web application</a> is often belied by the paucity of unique web addresses that it presents in the course of its use. Building the application in such a way that <a href="https://www.w3.org/2001/tag/doc/IdentifyingApplicationState#UseURIsforStates">distinct states are represented by distinct and fixed web addresses</a> permits users with a shared link to bypass arbitrary interactions to get to the desired destination, facilitates more precise citation, and provides a more granular <a href="https://www.w3.org/TR/annotation-model/#bodies-and-targets">target for annotation</a>. Critically, it also makes the website more accessible to both search engine and archival crawlers.</div>
      <h3>Use one link for each resource</h3>
      <div>Every web resource <a href="https://www.w3.org/TR/webarch/#id-resources">is available through at least one web address</a>. For archiving, it is additionally preferable that every web resource be available through <em>no more than</em> one web address. Archival crawlers often <a href="https://landsbokasafn.github.io/DeDuplicator/">de-duplicate captured content</a> based on a combination of web address and <a href="https://en.wikipedia.org/wiki/Checksum">checksum</a>. When either of those values varies from what was recorded in a previous crawl, the resource is considered new. Some content management systems allow for the same resource to be served using different web addresses, which will result in superfluous requests from crawlers and increased archival storage requirements.</div>
      <h3>Be careful with robots directives</h3>
      <div>You may already use the <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard">robots exclusion standard</a> to convey machine-readable preferences to search engine crawlers. <a href="https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/17590/Web%20Archiving%20in%20the%20United%20States_A%202017%20Survey.pdf#page=29">Most web archiving initiatives obey these instructions</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16">, at least conditionally. Directives that have historically been appropriate for search engine crawlers - e.g., <a href="../pdf/2014-04-19_building_archivable_websites.pdf#page=18">excluding directories containing scripts and style and layout instructions</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16"> - <a href="https://developers.google.com/search/blog/2014/10/updating-our-technical-webmaster">are becoming less so</a>. These exclusions have long been problematic in the archiving context, as they may prevent the capture of assets that are essential to faithfully re-presenting the archived website.</div>
      <br>
      <div>Aside from not discouraging the crawler from visiting vital resources, the robots exclusion standard can be affirmatively employed to improve archiving efforts. Use a site-level <a href="https://www.robotstxt.org/robotstxt.html"><span class="monospace">robots.txt</span></a> file to link to an <a href="https://www.sitemaps.org/">XML sitemap</a> or specify a sustainable <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard#Crawl-delay_directive">crawler request interval</a>. Liberally ward crawlers away from website sections <a href="https://en.wikipedia.org/wiki/Spider_trap">that may programmatically generate an arbitrary number of links</a> using a site-level <a href="https://www.robotstxt.org/robotstxt.html"><span class="monospace">robots.txt</span></a> file, a page-level <a href="https://www.robotstxt.org/meta.html"><span class="monospace">&lt;meta&gt;</span> tag</a>, or <a href="https://en.wikipedia.org/wiki/Nofollow"><span class="monospace">rel="nofollow"</span> link attributes</a>.</div>
      <h3>Mind content license terms</h3>
      <div>In the <a href="https://www.section108.gov/docs/Sec108StudyGroupReport.pdf#page=18">absence of specific provisions in U.S. copyright law to cover web content archiving by third-party cultural heritage institutions</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16">, U.S. web archiving organizations resort to <a href="https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/17590/Web%20Archiving%20in%20the%20United%20States_A%202017%20Survey.pdf#page=24">seeking explicit permission</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16"> and/or <a href="https://www.arl.org/storage/documents/publications/code-of-best-practices-fair-use.pdf#page=30">assertions of fair use</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16">. Boilerplate website terms of use or inadvertently strident copyright statements may deter archiving efforts by increasing the perception of legal risk. If you are amenable to the archiving of your web content, consider making it available under an <a href="https://en.wikipedia.org/wiki/Open_content">open content license</a> or at least ensure that terms of use and copyright statements are not antagonistic to archiving.</div>
      <h3>Return reliable response codes</h3>
      <div>Web archiving tools exhibit varying levels of success at <a href="https://github.com/internetarchive/heritrix3/blob/master/modules/src/main/java/org/archive/modules/extractor/ExtractorJS.java">reconstructing dynamically-generated web addresses from within JavaScript</a>. Sometimes, this process results in the generation of nonexistent web addresses. Configuring your web servers to return reliable <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP status codes</a> and avoiding <a href="https://en.wikipedia.org/wiki/HTTP_404#Soft_404">soft 404</a> responses, in particular, will facilitate detection and minimization of superfluous requests from archival and other crawlers.</div>
      <h3>Implement caching enhancements</h3>
      <div>Web clients including archival crawlers take advantage of various <a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Response_Headers">HTTP response headers</a> to minimize requests for content that hasn't changed since it was last cached: <a href="https://www.rfc-editor.org/rfc/rfc9110.html#name-content-length"><span class="monospace">Content-Length</span></a>, <a href="https://www.rfc-editor.org/rfc/rfc9110.html#name-last-modified"><span class="monospace">Last-Modified</span></a>, and <a href="https://en.wikipedia.org/wiki/HTTP_ETag"><span class="monospace">ETag</span></a>. Research suggests that <a data-originalurl="https://iwaw.europarchive.org/04/Clausen.pdf" data-versiondate="2011-07-22" href="https://web.archive.org/web/20110722001110/https://iwaw.europarchive.org/04/Clausen.pdf">server responses related to caching are not always reliable</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16">, yet the correct implementation of these HTTP headers will reduce superfluous requests from all types of clients.</div>
      <h3>Minimize reliance on external assets necessary for presentation</h3>
      <div>It is increasingly easy to take advantage of externally-hosted assets, such as <a href="https://developers.google.com/fonts">fonts</a> or <a href="https://developers.google.com/speed/libraries/">JavaScript libraries</a>. This can not only improve website performance and the availability and caching of those resources but also archiving, in a limited number of cases. For Internet Archive's <a href="https://help.archive.org/help/wayback-machine-general-information/">ongoing crawls of the public Web</a>, centrally-hosted files used by many different websites only need to be captured once. The rest of the web archiving community doesn't collect so broadly, however, and the major downside risk is that external hosts may be less disposed to archivability and <a href="../pdf/2014-04-19_building_archivable_websites.pdf#page=19">instruct crawlers not to collect assets</a> <img alt="pdf icon" height="16" src="../img/pdf_(16x16).png" width="16"> that may be necessary to faithfully re-present the archived website.</div>
      <h3>Serve reusable assets from a common location</h3>
      <div>The key rationale for hosting some resources externally - performance - should also motivate serving <a href="https://www.google.com/search?q=lanl+logo+site:lanl.gov&tbm=isch">reusable local assets</a> from a single location. Content management systems sometimes instantiate each new sub-site with its own complement of the standard theme assets. Storing these in a common location referenced by each of the sub-sites allows for more efficient client caching, simultaneously improving website performance and archivability.</div>
      <br>
      <div class="byline"><a href="https://nullhandle.org/web-archivability/capture.html" rel="canonical">Permalink</a> | Forked from an (archived) <a data-originalurl="https://library.stanford.edu/projects/web-archiving/archivability/capture" data-versiondate="2020-01-29" href="https://web.archive.org/web/20200129163112/https://library.stanford.edu/projects/web-archiving/archivability/capture">crosspost</a> to <a href="https://library.stanford.edu/">Stanford Libraries</a> <a href="https://library.stanford.edu/projects/web-archiving">Web Archiving</a><a href="https://creativecommons.org/licenses/by-sa/4.0/" rel="license"><img alt="Creative Commons Attribution-ShareAlike 4.0 International License" class="copyright" height="15" src="https://licensebuttons.net/l/by-sa/4.0/80x15.png" width="80"></a></div>
      <br>
      <script>
        var nav = responsiveNav(".nav-collapse");
      </script>
    </div>
  </body>
</html>